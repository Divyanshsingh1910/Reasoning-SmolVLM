{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "--> Run the cells step by step to achieve following results\n",
    "1. Get Reasoning traces from the LLM via API\n",
    "2. Structure the data in the LLM accepted format\n",
    "3. Create HuggingFace version of the data\n",
    "\n",
    "Note: Set the Gemini API keys in the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "import datasets\n",
    "IMG_DIR = \"data/filtered_images_5k/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Set the following paths ar per need**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = \"data/train_cot.json\"\n",
    "OUTPUT_HF_DIR = \"data/hf_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data/filtered_data_5k.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key_0 = os.get_env(\"GEMINI_API_KEY_0\") \n",
    "api_key_1 = os.get_env(\"GEMINI_API_KEY_1\")\n",
    "api_key_2 = os.get_env(\"GEMINI_API_KEY_2\")\n",
    "api_key_3 = os.get_env(\"GEMINI_API_KEY_3\")\n",
    "api_key_4 = os.get_env(\"GEMINI_API_KEY_4\")\n",
    "api_key_5 = os.get_env(\"GEMINI_API_KEY_5\")\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=api_key_0,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "client_1 = OpenAI(\n",
    "    api_key=api_key_1,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "client_2 = OpenAI(\n",
    "    api_key=api_key_2,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "client_3 = OpenAI(\n",
    "    api_key=api_key_3,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "client_4 = OpenAI(\n",
    "    api_key=api_key_4,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "client_5 = OpenAI(\n",
    "    api_key=api_key_5,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "# clients = [client, client_1, client_2, client_3, client_4, client_5]\n",
    "clients = [client_5, client_4, client_3, client_2, client_1, client]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"gemini-2.0-flash-thinking-exp-01-21\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a Vision Language Model specialized in interpreting visual data from chart images.\n",
    "\n",
    "## MAIN GOAL\n",
    "Your task is to analyze the provided chart image and respond to queries with concise answers, usually a single word, number, or short phrase.\n",
    "The charts include a variety of types (e.g., line charts, bar charts) and contain colors, labels, and text.\n",
    "Focus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.\n",
    "\n",
    "\n",
    "1. THINKING PROCESS\n",
    "- Use short, simple sentences that mirror natural thought patterns addressing the question.\n",
    "\n",
    "## OUTPUT FORMAT\n",
    "Your responses must follow this exact structure given below. Make sure to always include the final answer.\n",
    "\n",
    "```\n",
    "<think>\n",
    "[Your thinking process goes here]   <--- SHOULD NOT BE MORE THAN 2-3 LINES.\n",
    "</think>\n",
    "\n",
    "<answer>\n",
    "[The final answer to the question goes here] <--- should be very concise, usually a single word, number, or short phrase.\n",
    "</answer>\n",
    "\n",
    "<Example Responses>\n",
    "```\n",
    "1. <think>\\nThe user is asking to identify the largest bar in the chart.\\nComparing the heights of the bars visually, the 'Total revenue' bar is the tallest, indicating the greatest value.\\n</think>\\n\\n<answer>\\nTotal revenue\\n</answer>\n",
    "2. <think>\\nThe user wants the year with the minimum number of officers.\\nI need to find the lowest point on the line graph and read the corresponding year from the x-axis.\\nThe lowest value on the line is clearly above the '2013' label.\\n</think>\\n\\n<answer>\\n2013\\n</answer>\n",
    "3. <think>\\nThe query asks for the average of the navy blue bars.\\nFirst, identify the values of the four navy blue bars: 31, 47, 18, and 3.\\nSum these values (31+47+18+3=99) and divide by the number of bars (4) to get the average, which is 99/4 = 24.75.\\n</think>\\n\\n<answer>\\n24.75\\n</answer>\n",
    "4. <think>\\nThe user is asking for the total number of officers in 2015.\\nI need to find the bar corresponding to 2015 and read the value from the y-axis.\\nThe bar for 2015 is clearly labeled with a value of 50.\\n</think>\\n\\n<answer>\\n50\\n</answer>\n",
    "```\n",
    "\n",
    "## STYLE GUIDELINES ON THINKING PROCESS\n",
    "\n",
    "NOTE: Your response MUST follow the specified output format and the answers inside the <answer> tag should be concise, generally a single word, a numeric answer, an yes or no, or name/phrase.\n",
    "NOTE: Try to keep the thinking process VERY MUCH CONCISE and to the point, avoid unnecessary details.\n",
    "NOTE: For numeric answers don't answer in words, just give the number and for fractions simply to write in decimal\n",
    "NOTE: When two words/phrases are the answer, separate them by a comma. For example: \"yes, no\", \"China, India\", \"2015, 2016\"\n",
    "\n",
    "NOTE: The reasoning process inside <think> tags SHOULD STRICTLY BE UNDER 3 LINES. BASICALLY, it should be extremely to the point, concise and nothing unnecessary.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_lm(prompt, image_path=None, api_client = None):\n",
    "    if api_client is None:\n",
    "        api_client = client\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": SYSTEM_PROMPT\n",
    "        }\n",
    "    ]\n",
    "    prompt = {\"type\": \"text\", \"text\": prompt}\n",
    "    user_content = [prompt]\n",
    "\n",
    "    if image_path:\n",
    "        try:\n",
    "            encoded_image = encode_image(image_path)\n",
    "\n",
    "            user_content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{encoded_image}\"  # Assuming JPEG, adjust if needed\n",
    "                }\n",
    "            })\n",
    "        except FileNotFoundError:\n",
    "            return \"Error: Image file not found.\"\n",
    "        except Exception as e:\n",
    "            return f\"Error reading image file: {e}\"\n",
    "\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_content\n",
    "    })\n",
    "\n",
    "    response = api_client.chat.completions.create(\n",
    "        model=model_id,\n",
    "        messages=messages,\n",
    "    )\n",
    "    # print(response)\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OUTPUT = []\n",
    "FAILED = []\n",
    "ERRORS = []\n",
    "CLIENT_COUNT = [0] * len(clients)\n",
    "cnt = 0\n",
    "x = 0\n",
    "client_id = 0\n",
    "NUM_CLIENTS = len(clients)\n",
    "for item in data[x:]:\n",
    "    qs = item[\"query\"]\n",
    "    img = IMG_DIR + item[\"imgname\"]\n",
    "\n",
    "    response = None\n",
    "    # clients = [client, client_1, client_2, client_3, client_4, client_5]\n",
    "    client = clients[client_id]\n",
    "\n",
    "    try_cnt = 0\n",
    "\n",
    "    while True:\n",
    "        if try_cnt >= len(clients):\n",
    "            FAILED.append(item)\n",
    "            break \n",
    "        else:\n",
    "            try_cnt += 1\n",
    "            try:\n",
    "                client = clients[client_id]\n",
    "                response = ask_lm(qs, img, client)\n",
    "                CLIENT_COUNT[client_id] += 1\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Client {client_id} failed: {e}\")\n",
    "                client_id += 1\n",
    "                client_id %= NUM_CLIENTS\n",
    "\n",
    "    if response is not None:\n",
    "        item[\"output\"] = response\n",
    "        OUTPUT.append(item)\n",
    "    cnt += 1\n",
    "    if cnt%25 == 0:\n",
    "        print(f\"{cnt} calls done\")\n",
    "        # write output to file\n",
    "        with open(\"data/train_smolcot_left2k_\" + str(x) + \"_on.json\", \"w\") as f:\n",
    "            json.dump(OUTPUT, f, indent = 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    json.dump(OUTPUT, f, indent = 4)\n",
    "\n",
    "train_data = OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total calls:\", cnt)\n",
    "print(\"Total successful calls:\", len(OUTPUT))\n",
    "print(\"Total failed calls:\", len(FAILED))\n",
    "print(\"Total errors:\", len(ERRORS))\n",
    "\n",
    "for i in range(len(clients)):\n",
    "    print(f\"Client {i} calls:\", CLIENT_COUNT[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/divyansh/Desktop/acads/sem-8/cs776/project/.project_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, Sequence, Image as HFImageFeature\n",
    "from PIL import Image\n",
    "import json, os\n",
    "def transform_entry(data_entry, image_dir):\n",
    "\n",
    "    img_filename = data_entry.get('imgname')\n",
    "    query = data_entry.get('query')\n",
    "    output = data_entry.get('output')\n",
    "\n",
    "    if not all([img_filename, query, output]):\n",
    "        print(f\"Warning: Skipping entry due to missing fields: {data_entry}\")\n",
    "        return None, None\n",
    "\n",
    "    img_path = os.path.join(image_dir, img_filename)\n",
    "\n",
    "    try:\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        # img_bytes = image[\"bytes\"]\n",
    "        # pil_image = PIL.Image.open(io.BytesIO(img_bytes))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Image file not found at {img_path}. Skipping entry.\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error loading image {img_path}: {e}. Skipping entry.\")\n",
    "        return None, None\n",
    "\n",
    "    # Construct the 'messages' field according to the required structure\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": query},\n",
    "                # 'index': 0 refers to the first image in the 'images' list below\n",
    "                # 'text': None as seen in the example for image placeholders\n",
    "                {\"type\": \"image\", \"index\": 0, \"text\": None}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": output, \"index\": None}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # a = {}\n",
    "    # a[\"image\"] = image\n",
    "    # a[\"messages\"] = messages\n",
    "    # return a\n",
    "    # return {\"images\": [image], \"messages\": messages}\n",
    "    return image, messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting transformation for 300 entries...\n",
      "Processing entry 1/300: 12756.png\n"
     ]
    }
   ],
   "source": [
    "transformed_data_list = []\n",
    "images = []\n",
    "messages = []\n",
    "print(f\"Starting transformation for {len(train_data)} entries...\")\n",
    "for i, entry in enumerate(train_data):\n",
    "    if i%300==0:\n",
    "      print(f\"Processing entry {i+1}/{len(train_data)}: {entry.get('imgname')}\")\n",
    "    img, msg = transform_entry(entry, IMG_DIR)\n",
    "    if img is None or msg is None:\n",
    "        continue\n",
    "    images.append(img)\n",
    "    messages.append(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.Dataset.from_dict(\n",
    "    {'images': images, 'messages': messages},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Image as HFImageFeature, Sequence\n",
    "\n",
    "def wrap_image_in_list(batch):\n",
    "  \"\"\"\n",
    "  Takes a batch dictionary and wraps each image in the 'images' list\n",
    "  into its own single-element list.\n",
    "  \"\"\"\n",
    "  batch['images'] = [[img] for img in batch['images']]\n",
    "  return batch # Return the modified batch\n",
    "\n",
    "\n",
    "\n",
    "new_image_feature = Sequence(feature=HFImageFeature(decode=True))\n",
    "\n",
    "def transform(dataset):\n",
    "    transformed_dataset = dataset.map(\n",
    "        wrap_image_in_list,\n",
    "        batched=True\n",
    "    )\n",
    "    transformed_dataset = transformed_dataset.cast_column(\"images\", new_image_feature)\n",
    "    transformed_dataset.features\n",
    "    return transformed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 300/300 [00:27<00:00, 11.08 examples/s]\n",
      "Casting the dataset: 100%|██████████| 300/300 [00:00<00:00, 70299.53 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'images': [<PIL.PngImagePlugin.PngImageFile image mode=RGB size=204x394>],\n",
       " 'messages': [{'content': [{'index': None,\n",
       "     'text': 'Is the value of \"Don\\'t know\" segment 7%?',\n",
       "     'type': 'text'},\n",
       "    {'index': 0, 'text': None, 'type': 'image'}],\n",
       "   'role': 'user'},\n",
       "  {'content': [{'index': None,\n",
       "     'text': '<think>\\nThe user is asking to check if the value of \"Don\\'t know\" segment is 7%.\\nLooking at the pie chart, the \"Don\\'t know\" segment is clearly labeled with \"7%\".\\nTherefore, the statement is correct.\\n</think>\\n\\n<answer>\\nYes\\n</answer>',\n",
       "     'type': 'text'}],\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data = transform(dataset)\n",
    "final_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.save_to_disk(OUTPUT_HF_DIR)\n",
    "print(\"Dataset saved to disk.\")2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
