{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Instructions\n",
        "Choose anyone of the evaluater's name and comment other evaluater's name\n",
        "- Assign the response file for all the models accordingly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrO9gbk43kxY"
      },
      "outputs": [],
      "source": [
        "def get_model_responses(index, model):\n",
        "    \n",
        "    file = None\n",
        "    if model == \"256M-base\":\n",
        "        file = \"results/base_256M_eval_results.json\"\n",
        "    elif model == \"256M-base+step\":\n",
        "        file = \"results/base_256M_eval_think_results.json\"\n",
        "    elif model == \"500M-base\":\n",
        "        file = \"results/base_500M_eval_results.json\"\n",
        "    elif model == \"500M-base+step\":\n",
        "        file = \"results/base_500M_eval_think_results.json\"\n",
        "    elif model == \"256M-3k\":\n",
        "        file = \"results/sft_256M_3k_eval_results.json\"\n",
        "    elif model == \"256M-3k+step\":\n",
        "        file = \"results/sft_256M_3k_eval_think_results.json\"\n",
        "    elif model == \"256M-7k\":\n",
        "        file = \"results/sft_256M_7k_eval_results.json\"\n",
        "    elif model == \"256M-7k+step\":\n",
        "        file = \"results/sft_256M_7k_eval_think_results.json\"\n",
        "    elif model == \"500M-3k\":\n",
        "        file = \"results/sft_500M_3k_eval_results.json\"\n",
        "    elif model == \"500M-3k+step\":\n",
        "        file = \"results/sft_500M_3k_think_eval_results.json\"\n",
        "    elif model == \"500M-7k\":\n",
        "        file = \"results/sft_500M_7k_eval_results.json\"\n",
        "    elif model == \"500M-7k+step\":\n",
        "        file = \"results/sft_500M_7k_eval_think_results.json\"\n",
        "    else:\n",
        "        raise Exception(\"Model not found\")\n",
        "    \n",
        "    with open(file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    \n",
        "    return data[index][\"output\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img_folder_path = '../data/filtered_images_5k'  # ye path set karna hoga"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wN_gJdgH5YxR"
      },
      "outputs": [],
      "source": [
        "models = ['256M-base', '256M-base+step', '256M-3k', '256M-3k+step', \n",
        "          '256M-7k', '256M-7k+step',\n",
        "          '500M-base', '500M-base+step', '500M-3k', '500M-3k+step', \n",
        "          '500M-7k', '500M-7k+step']\n",
        "# all combinations of models\n",
        "configurations = []\n",
        "num_models = len(models)\n",
        "for i in range(num_models):\n",
        "    for j in range(i + 1, num_models):\n",
        "        configurations.append((models[i], models[j]))\n",
        "\n",
        "print(len(configurations))\n",
        "# evaluater_name = \"Divyansh\"\n",
        "# config = configurations[0:20]\n",
        "\n",
        "evaluater_name = \"Sandy\"\n",
        "config = configurations[20:40]\n",
        "\n",
        "# evaluater_name = \"Aniket\"\n",
        "# config = configurations[40:50]\n",
        "\n",
        "# evaluater_name = \"Apoorva\"\n",
        "# config = configurations[50:58]\n",
        "\n",
        "# evaluater_name = \"Anuj\"\n",
        "# config = configurations[58:66]\n",
        "\n",
        "print(len(config))\n",
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWDTqjIf4UN5",
        "outputId": "eec0dd1c-4f3c-4d13-bc70-acf8ec408589"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "import random\n",
        "queries_path = 'test_set_answers.json'\n",
        "responses_path = './'\n",
        "with open(queries_path, 'r') as f:\n",
        "  queries = json.load(f)\n",
        "print(queries[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_and_get_choice(img, query, answer, responseA, responseB):\n",
        "    clear_output(wait=True)\n",
        "    # Display image\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Image for the Query\")\n",
        "\n",
        "    # Display query and responses\n",
        "    print(f\"\\nQuery: {query}\")\n",
        "    print(f\"True ans for reference: {answer}\")\n",
        "    print(f\"\\nModel A Response:\\n{responseA}\")\n",
        "    print(f\"\\nModel B Response:\\n{responseB}\")\n",
        "    plt.show()\n",
        "\n",
        "    # time.sleep(0.1)\n",
        "    # Create buttons\n",
        "    resp = int(input(\"Better? (0: Tie / 1: Model A / 2: Model B/ -1: pause)\\n\").strip())\n",
        "\n",
        "    return resp\n",
        "\n",
        "f = open(f\"{responses_path}/responses_{evaluater_name}.json\", 'a')\n",
        "\n",
        "used_indices = set()\n",
        "# multiple session mein ye overwrite nhi hona chahiye"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "id": "ML8ye_mQCdnh",
        "outputId": "7dcfe6f1-dcdc-4694-b927-5f68326a3007"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ek baar mein 20 session lga rha hun\n",
        "count = 0\n",
        "while count < 20:\n",
        "    index = random.randint(0, len(queries) - 1)\n",
        "    while index < len(queries) and index in used_indices:\n",
        "        index = random.randint(0, len(queries) - 1)\n",
        "\n",
        "    used_indices.add(index)\n",
        "    count += 1\n",
        "    for c in configurations:\n",
        "        clear_output(wait=True)\n",
        "        modelA, modelB = c\n",
        "\n",
        "        if random.random() < 0.5:\n",
        "            modelA, modelB = modelB, modelA  # Swap\n",
        "        responseA = get_model_responses(index, modelA)\n",
        "        responseB = get_model_responses(index, modelB)\n",
        "        query = queries[index]\n",
        "        answer = queries[index][\"label\"]\n",
        "        img = Image.open(f\"{img_folder_path}/{query['imgname']}\")\n",
        "        choice = show_and_get_choice(img, query['query'], answer, responseA, responseB)\n",
        "        if choice == -1:\n",
        "            index = len(queries)\n",
        "            # exit(0)\n",
        "            count = 20\n",
        "            break\n",
        "        eval_result = {\n",
        "            'index': index,\n",
        "            'model1': modelA,\n",
        "            'model2': modelB,\n",
        "            'response1': responseA,\n",
        "            'response2': responseB,\n",
        "            'choice': choice\n",
        "        }\n",
        "        f.write(json.dumps(eval_result) + \"\\n\")\n",
        "        f.flush()\n",
        "\n",
        "        index += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instructions\n",
        "Put the `response_files` accordingly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9h6cfea9USS",
        "outputId": "526beac7-b3ad-4ff7-91c8-d91bff886614"
      },
      "outputs": [],
      "source": [
        "response_files = [\n",
        "    'votes/responses_Divyansh.json', \n",
        "    'votes/responses_Aniket.json', \n",
        "    'votes/responses_Anuj.json', \n",
        "    'votes/responses_Apoorva.json',\n",
        "    'votes/responses_Sandy.json',\n",
        "    ]\n",
        "\n",
        "all_comparisons = []\n",
        "for fname in response_files:\n",
        "    with open(f\"{responses_path}/{fname}\", 'r') as f:\n",
        "        for line in f:\n",
        "            eval_result = json.loads(line)\n",
        "            all_comparisons.append(eval_result)\n",
        "\n",
        "# Elo rating calculation\n",
        "def calculate_elo(rating1, rating2, result, k=32):\n",
        "    \"\"\"Calculate Elo ratings after a game result\"\"\"\n",
        "    expected_score_p1 = 1 / (1 + 10 ** ((rating2 - rating1) / 400))\n",
        "    expected_score_p2 = 1 / (1 + 10 ** ((rating1 - rating2) / 400))\n",
        "\n",
        "    # Result values: 1 = player 1 wins, 0 = player 2 wins, 0.5 = draw\n",
        "    if result == \"p1 wins\":\n",
        "        actual_score_p1, actual_score_p2 = 1, 0\n",
        "    elif result == \"p2 wins\":\n",
        "        actual_score_p1, actual_score_p2 = 0, 1\n",
        "    else:\n",
        "        actual_score_p1, actual_score_p2 = 0.5, 0.5\n",
        "\n",
        "    # Calculate new ratings\n",
        "    new_rating_p1 = rating1 + k * (actual_score_p1 - expected_score_p1)\n",
        "    new_rating_p2 = rating2 + k * (actual_score_p2 - expected_score_p2)\n",
        "\n",
        "    return new_rating_p1, new_rating_p2\n",
        "\n",
        "# Function to calculate Elo ratings for all players\n",
        "def calculate_elo_ratings(game_results, initial_rating=1500, k=32):\n",
        "    player_ratings = {}\n",
        "\n",
        "    for game in game_results:\n",
        "        p1, p2, result = game\n",
        "\n",
        "        # Set initial ratings if players haven't played before\n",
        "        if p1 not in player_ratings:\n",
        "            player_ratings[p1] = initial_rating\n",
        "        if p2 not in player_ratings:\n",
        "            player_ratings[p2] = initial_rating\n",
        "\n",
        "        # Get current ratings\n",
        "        rating_p1 = player_ratings[p1]\n",
        "        rating_p2 = player_ratings[p2]\n",
        "\n",
        "        # Calculate new ratings\n",
        "        new_rating_p1, new_rating_p2 = calculate_elo(rating_p1, rating_p2, result, k)\n",
        "\n",
        "        # Update player ratings\n",
        "        player_ratings[p1] = new_rating_p1\n",
        "        player_ratings[p2] = new_rating_p2\n",
        "\n",
        "    return player_ratings\n",
        "\n",
        "# Example usage\n",
        "def get_result(choice):\n",
        "    if choice == 0:\n",
        "        return \"tie\"\n",
        "    elif choice == 1:\n",
        "        return \"p1 wins\"\n",
        "    else:\n",
        "        return \"p2 wins\"\n",
        "game_results = [(c['model1'], c['model2'], get_result(c['choice'])) for c in all_comparisons]\n",
        "print(len(game_results))\n",
        "elo_ratings = calculate_elo_ratings(game_results)\n",
        "for player, rating in elo_ratings.items():\n",
        "    print(f\"{player}: {rating:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sorted_ratings = sorted(elo_ratings.items(), key=lambda item: item[1])\n",
        "\n",
        "# Print the sorted items\n",
        "for key, value in sorted_ratings:\n",
        "    print(f\"{key}: {value}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".project_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
